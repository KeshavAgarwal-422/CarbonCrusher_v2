{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\kesha\\\\Desktop\\\\My Documents\\\\dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8821034775233249\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bus       0.79      0.93      0.85       189\n",
      "         Car       0.86      0.78      0.82       243\n",
      "       Still       0.93      0.94      0.94       248\n",
      "       Train       0.86      0.83      0.85       243\n",
      "     Walking       0.94      0.94      0.94       256\n",
      "\n",
      "    accuracy                           0.88      1179\n",
      "   macro avg       0.88      0.88      0.88      1179\n",
      "weighted avg       0.88      0.88      0.88      1179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[175   5   0   5   4]\n",
      " [ 26 189   8  17   3]\n",
      " [  2   3 234   7   2]\n",
      " [  9  19   8 202   5]\n",
      " [  9   3   1   3 240]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Target: ['Train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Replace the 'new_sensor_readings' variable with your actual sensor readings\n",
    "new_sensor_readings = [[17,\t9.728209279\t,9.111534979\t,10.15468033,\t0.210600143\t,0,\t0\t,0,\t0,\t0,\t0\t,0\t,0\n",
    "\n",
    "]]\n",
    "\n",
    "prediction = model.predict(new_sensor_readings)\n",
    "print(\"Predicted Target:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted mode of transportation is: Bus\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predict_transport_mode(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Define the feature columns\n",
    "    features = ['android.sensor.accelerometer#mean', \n",
    "                'android.sensor.accelerometer#min', \n",
    "                'android.sensor.accelerometer#max', \n",
    "                'android.sensor.gyroscope#mean', \n",
    "                'android.sensor.gyroscope#min', \n",
    "                'android.sensor.gyroscope#max']\n",
    "\n",
    "    # Define the target column\n",
    "    target = ['target']\n",
    "\n",
    "    # Encode the target column\n",
    "    le = LabelEncoder()\n",
    "    data['target'] = le.fit_transform(data['target'])\n",
    "\n",
    "    # Split the data into a training set and a test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and fit the Random Forest classifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Prompt the user to enter their own sensor readings\n",
    "    accelerometer_mean = float(input(\"Enter the mean accelerometer reading: \"))\n",
    "    accelerometer_min = float(input(\"Enter the min accelerometer reading: \"))\n",
    "    accelerometer_max = float(input(\"Enter the max accelerometer reading: \"))\n",
    "    gyroscope_mean = float(input(\"Enter the mean gyroscope reading: \"))\n",
    "    gyroscope_min = float(input(\"Enter the min gyroscope reading: \"))\n",
    "    gyroscope_max = float(input(\"Enter the max gyroscope reading: \"))\n",
    "\n",
    "    # Create a DataFrame from the user's input\n",
    "    user_input = pd.DataFrame([[accelerometer_mean, accelerometer_min, accelerometer_max, gyroscope_mean, gyroscope_min, gyroscope_max]], columns=features)\n",
    "\n",
    "    # Use the trained model to predict the mode of transportation\n",
    "    prediction = rf.predict(user_input)\n",
    "\n",
    "    # Decode the prediction\n",
    "    prediction = le.inverse_transform(prediction)\n",
    "\n",
    "    # Print the predicted mode of transportation\n",
    "    print(\"The predicted mode of transportation is: \" + prediction[0])\n",
    "\n",
    "# Use the function\n",
    "predict_transport_mode('C:\\\\Users\\\\kesha\\\\Desktop\\\\My Documents\\\\dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'perceptron', 'squared_error', 'hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber', 'log_loss', 'squared_hinge'}. Got 'log' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     features_chunk \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(features_chunk)\n\u001b[0;32m     39\u001b[0m     \u001b[39m# Train the SGDClassifier\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     sgd_clf\u001b[39m.\u001b[39;49mpartial_fit(features_chunk, target_chunk, classes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49munique(target_chunk))\n\u001b[0;32m     42\u001b[0m \u001b[39m# Check the performance of the model on the last chunk of data\u001b[39;00m\n\u001b[0;32m     43\u001b[0m y_pred_chunk \u001b[39m=\u001b[39m sgd_clf\u001b[39m.\u001b[39mpredict(features_chunk)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'perceptron', 'squared_error', 'hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber', 'log_loss', 'squared_hinge'}. Got 'log' instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SGDClassifier\n",
    "sgd_clf = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize a LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# We'll read the data in chunks, preprocess each chunk, and train the model\n",
    "chunksize = 10 ** 5\n",
    "i = 0\n",
    "for chunk in pd.read_csv(r'C:\\Users\\kesha\\Downloads\\cleaned.csv\\cleaned.csv', chunksize=chunksize):\n",
    "    print(f'Processing chunk {i}')\n",
    "    i += 1\n",
    "    \n",
    "    # Sample a fraction of the data to manage memory\n",
    "    chunk_sample = chunk.sample(frac=0.05, random_state=42)\n",
    "\n",
    "    # Extract features and target\n",
    "    features_chunk = chunk_sample[['x', 'y', 'z']]\n",
    "    target_chunk = chunk_sample['class']\n",
    "    \n",
    "    # If this is the first chunk, fit the LabelEncoder, otherwise, transform only\n",
    "    if i == 1:\n",
    "        target_chunk = le.fit_transform(target_chunk)\n",
    "    else:\n",
    "        target_chunk = le.transform(target_chunk)\n",
    "    \n",
    "    # Scale the features\n",
    "    features_chunk = scaler.fit_transform(features_chunk)\n",
    "    \n",
    "    # Train the SGDClassifier\n",
    "    sgd_clf.partial_fit(features_chunk, target_chunk, classes=np.unique(target_chunk))\n",
    "\n",
    "# Check the performance of the model on the last chunk of data\n",
    "y_pred_chunk = sgd_clf.predict(features_chunk)\n",
    "accuracy = accuracy_score(target_chunk, y_pred_chunk)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 34.5 MiB for an array with shape (4522442,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m clf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     32\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     33\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1854\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1857\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1784\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1785\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1786\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:295\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    293\u001b[0m y_encoded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(y\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_):\n\u001b[1;32m--> 295\u001b[0m     classes_k, y_encoded[:, k] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(y[:, k], return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    296\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mappend(classes_k)\n\u001b[0;32m    297\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\u001b[39m.\u001b[39mappend(classes_k\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:328\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unique1d\u001b[39m(ar, return_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_inverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    324\u001b[0m               return_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, equal_nan\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    Find the unique elements of an array, ignoring shape.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    330\u001b[0m     optional_indices \u001b[39m=\u001b[39m return_index \u001b[39mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m optional_indices:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 34.5 MiB for an array with shape (4522442,) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r'C:\\Users\\kesha\\Downloads\\reduced_cleaned.csv')\n",
    "\n",
    "# Prepare the features and the target variable\n",
    "X = data[['x', 'y', 'z']]\n",
    "y = data['class']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature variables\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
